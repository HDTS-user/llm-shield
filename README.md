# llm-shield
This solution predicts and prevents jailbreak attempts, ensuring your prompts remain safe from injection attacks.
